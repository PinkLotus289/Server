import os
import json
import time
import logging
import tempfile
import multiprocessing as mp
from multiprocessing import Process, Queue

from httpx import ProxyError
from urllib3.exceptions import ProtocolError
from selenium.common.exceptions import NoSuchWindowException

from .fetcher import IaaIFetcher
from .parser import parse_items
from .utils import sleep_random

# –ü–∞–ø–∫–∞ –¥–ª—è –ª–æ–≥–æ–≤ –≤–Ω—É—Ç—Ä–∏ parser/
BASE_DIR = os.path.dirname(__file__)
LOG_DIR = os.path.join(BASE_DIR, "logs")
os.makedirs(LOG_DIR, exist_ok=True)

def ready_flag_path(keyword: str) -> str:
    """–ü—É—Ç—å –∫ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–ª–∞–≥—É –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ —Å–µ–∫—Ü–∏–∏."""
    return os.path.join(tempfile.gettempdir(), f"parser_ready_{keyword}")

def setup_logger(keyword: str) -> logging.Logger:
    logger = logging.getLogger(keyword)
    logger.setLevel(logging.INFO)
    if not logger.handlers:
        fmt = logging.Formatter("%(asctime)s [%(name)s] %(levelname)s: %(message)s")
        sh = logging.StreamHandler()
        sh.setFormatter(fmt)
        logger.addHandler(sh)
        fh = logging.FileHandler(os.path.join(LOG_DIR, f"{keyword}.log"), encoding="utf-8")
        fh.setFormatter(fmt)
        logger.addHandler(fh)
    return logger

def process_section(
    keyword: str,
    proxy_port: int,
    output_dir: str,
    start_page: int,
    page_size: int,
    logger: logging.Logger
) -> int:
    fetcher = IaaIFetcher(keyword, page_size=page_size, proxy_port=proxy_port)
    dyn_pages = fetcher.start_session()
    logger.info(f"–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —á–∏—Å–ª–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {dyn_pages}")

    # —Å—Ç–∞–≤–∏–º —Ñ–ª–∞–≥ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –∑–∞–∫—Ä—ã—Ç–∏—è –±—Ä–∞—É–∑–µ—Ä–∞
    open(ready_flag_path(keyword), "w").close()

    client = fetcher.build_client()
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, f"{keyword}_lots.json")

    # –∑–∞–≥—Ä—É–∂–∞–µ–º —É–∂–µ —Å–æ–±—Ä–∞–Ω–Ω–æ–µ, –µ—Å–ª–∏ –µ—Å—Ç—å
    if os.path.exists(output_path):
        try:
            with open(output_path, "r", encoding="utf-8") as f:
                all_items = json.load(f)
        except json.JSONDecodeError:
            all_items = []
    else:
        all_items = []

    for page in range(start_page, dyn_pages + 1):
        logger.info(f"–ó–∞–ø—Ä–æ—Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}/{dyn_pages}")
        html = fetcher.fetch_page(client, page)

        if "captcha" in html.lower() or "incapsula" in html.lower():
            logger.warning(f"–ö–∞–ø—á–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page}, –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫ —Å–µ—Å—Å–∏–∏")
            raise RuntimeError("Captcha detected")

        items = parse_items(html)
        logger.info(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: –Ω–∞–π–¥–µ–Ω–æ {len(items)} –ª–æ—Ç–æ–≤")
        all_items.extend(items)

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(all_items, f, ensure_ascii=False, indent=2)
        logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(all_items)} –ª–æ—Ç–æ–≤ ‚Üí {output_path}")

        sleep_random(2, 5)

    logger.info(f"üéâ –í—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã ({dyn_pages}) –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã.")
    return dyn_pages

def run_section_with_restart(
    keyword: str,
    proxy_port: int,
    output_dir: str,
    page_size: int,
    max_retries: int = 5
) -> str:
    logger = setup_logger(keyword)
    output_path = os.path.join(output_dir, f"{keyword}_lots.json")

    # –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ä—Ç–æ–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
    if os.path.exists(output_path):
        try:
            with open(output_path, "r", encoding="utf-8") as f:
                current = json.load(f)
            done_pages = len(current) // page_size
            start_page = done_pages + 1
            logger.info(f"–ù–∞–π–¥–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª, —Å—Ç–∞—Ä—Ç —Å {start_page}")
        except json.JSONDecodeError:
            start_page = 1
            logger.warning(f"–§–∞–π–ª {output_path} –±–∏—Ç—ã–π, —Å—Ç–∞—Ä—Ç—É–µ–º —Å 1")
    else:
        start_page = 1

    attempts = 0
    while True:
        try:
            process_section(keyword, proxy_port, output_dir, start_page, page_size, logger)
            break

        except ProxyError as e:
            logger.error(f"ProxyError –Ω–∞ —Ä–∞–∑–¥–µ–ª–µ ¬´{keyword}¬ª: {e}")
            logger.error("–ü—Ä–æ–∫—Å–∏ —Ç—Ä–µ–±—É–µ—Ç –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—é ‚Äî –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º.")
            break

        except (ProtocolError, NoSuchWindowException) as e:
            attempts += 1
            logger.error(f"{type(e).__name__} –Ω–∞ —Ä–∞–∑–¥–µ–ª–µ ¬´{keyword}¬ª: {e}")
            if attempts >= max_retries:
                logger.error(f"–ü—Ä–µ–≤—ã—à–µ–Ω–æ {max_retries} –ø–æ–ø—ã—Ç–æ–∫ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞ –¥–ª—è ¬´{keyword}¬ª, –≤—ã—Ö–æ–¥–∏–º.")
                break
            logger.info(f"–ü–æ–ø—ã—Ç–∫–∞ {attempts}/{max_retries} –ø–æ—Å–ª–µ —Å–±–æ—è, —Ä–µ—Å—Ç–∞—Ä—Ç —á–µ—Ä–µ–∑ 5 —Å–µ–∫")
            time.sleep(5)
            continue

        except Exception as e:
            attempts += 1
            logger.exception(f"–°–±–æ–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {start_page}: {e}")
            if attempts >= max_retries:
                logger.error(f"–ü—Ä–µ–≤—ã—à–µ–Ω–æ {max_retries} –ø–æ–ø—ã—Ç–æ–∫ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞ –¥–ª—è ¬´{keyword}¬ª, –≤—ã—Ö–æ–¥–∏–º.")
                break
            # –ø–µ—Ä–µ—Å—á—ë—Ç —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            if os.path.exists(output_path):
                try:
                    with open(output_path, "r", encoding="utf-8") as f:
                        current = json.load(f)
                    done_pages = len(current) // page_size if current else 0
                    start_page = done_pages + 1
                except json.JSONDecodeError:
                    start_page = 1
            else:
                start_page = 1
            logger.info(f"–ü–æ–ø—ã—Ç–∫–∞ {attempts}/{max_retries}. –†–µ—Å—Ç–∞—Ä—Ç —Å {start_page} —á–µ—Ä–µ–∑ 5 —Å–µ–∫")
            time.sleep(5)

    return output_path

def worker(keyword: str, proxy_port: int, output_dir: str, page_size: int, queue: Queue):
    try:
        result = run_section_with_restart(keyword, proxy_port, output_dir, page_size)
        queue.put(result)
    except Exception as e:
        setup_logger(keyword).exception(f"–§–∞—Ç–∞–ª—å–Ω—ã–π —Å–±–æ–π —Ä–∞–∑–¥–µ–ª–∞: {e}")

def main():
    mp.set_start_method("spawn", force=True)

    cfg_path = os.path.join(BASE_DIR, "sections.json")
    cfg = json.load(open(cfg_path, "r", encoding="utf-8"))
    sections   = cfg.get("sections", [])
    output_dir = cfg.get("output_dir", "JSONs")
    page_size  = cfg.get("page_size", 100)

    queue = Queue()
    processes = []

    for idx, sec in enumerate(sections):
        keyword, port = sec["keyword"], sec["proxy_port"]
        p = Process(target=worker,
                    args=(keyword, port, output_dir, page_size, queue),
                    name=keyword)
        p.start()
        setup_logger(keyword).info(f"Started process pid={p.pid}")

        # –∂–¥—ë–º —Ñ–ª–∞–≥ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∏ –¥–∞—ë–º –µ—â—ë 120—Å
        flag = ready_flag_path(keyword)
        setup_logger(keyword).info("Waiting for browser init‚Ä¶")
        while not os.path.exists(flag):
            time.sleep(1)
        os.remove(flag)
        if idx < len(sections) - 1:
            setup_logger(keyword).info("Browser init done ‚Äî sleeping 120s before next")
            time.sleep(60)

        processes.append(p)

    for p in processes:
        p.join()
        setup_logger(p.name).info(f"Process {p.name} pid={p.pid} exited with code={p.exitcode}")

    # —Å–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results = []
    while not queue.empty():
        results.append(queue.get())

    print("–í—Å–µ –∑–∞–¥–∞—á–∏ –∑–∞–≤–µ—Ä—à–µ–Ω—ã. –§–∞–π–ª—ã:")
    for r in results:
        print(" ", r)

if __name__ == "__main__":
    main()
