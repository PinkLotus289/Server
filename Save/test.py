#!/usr/bin/env python3
import json
import random
import httpx
import sys
import re
import urllib.parse
from pathlib import Path
from bs4 import BeautifulSoup

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
# Residential proxy settings (pl.decodo.com ports 20001‚Äì20010)
PROXY_USER = "spz49ysjcr"
PROXY_PASS = "jB3fl2=arzsTd3OK8m"
PROXY_HOST = "pl.decodo.com"
PROXY_PORT = 20000
PROXY_URL = f"http://{PROXY_USER}:{PROXY_PASS}@{PROXY_HOST}:{PROXY_PORT}"
# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî

def random_ipv4() -> str:
    return ".".join(str(random.randint(1, 255)) for _ in range(4))

def get_client() -> httpx.Client:
    headers = {
        "User-Agent": "Mozilla/5.0 (compatible; iaai-lot-parser/1.0)",
        "X-Forwarded-For": random_ipv4(),
    }
    transport = httpx.HTTPTransport(proxy=PROXY_URL, retries=1)
    return httpx.Client(transport=transport, headers=headers, timeout=20.0)

def fetch_html(stock_id: str, out_path: Path) -> str:
    """
    –°–∫–∞—á–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –ª–æ—Ç–∞ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ out_path.
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤–Ω–µ—à–Ω–∏–π IP.
    """
    url = f"https://www.iaai.com/VehicleDetail/{stock_id}~US"
    with get_client() as client:
        try:
            ip = client.get("https://api64.ipify.org?format=text").text.strip()
        except Exception:
            ip = "unknown"
        r = client.get(url); r.raise_for_status()
        out_path.write_text(r.text, encoding="utf-8")
    return ip

def fetch_vehicle_ajax(salvage_id: str) -> dict:
    """
    AJAX-–∑–∞–ø—Ä–æ—Å /Home/GetVehicleData –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ VIN –∏ –±–∞–∑–æ–≤—ã—Ö –ø–æ–ª–µ–π.
    """
    url = f"https://vis.iaai.com/Home/GetVehicleData?salvageId={salvage_id}"
    headers = {
        "X-Requested-With": "XMLHttpRequest",
        "X-Forwarded-For": random_ipv4(),
    }
    with get_client() as client:
        r = client.get(url, headers=headers); r.raise_for_status()
        return r.json()

def fetch_detail_api(stock_id: str) -> dict:
    """
    –°–∫—Ä—ã—Ç—ã–π API /api/Search/GetDetailPageData
    –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å–µ –ø–æ–ª—è –ª–æ—Ç–∞ + –º–∞—Å—Å–∏–≤ –∫–∞—Ä—Ç–∏–Ω–æ–∫.
    """
    url = "https://www.iaai.com/api/Search/GetDetailPageData"
    params = {"stockNumber": stock_id}
    with get_client() as client:
        r = client.get(url, params=params); r.raise_for_status()
        return r.json()

def parse_html(path: Path) -> dict:
    """
    –ü–∞—Ä—Å–∏–º vehicle.html –∏ –≤—ã—Ç–∞—Å–∫–∏–≤–∞–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø–æ–ª—è –ø–æ –ª–µ–π–±–ª–∞–º
    data-list__label ‚Üí data-list__value, –≤–∫–ª—é—á–∞—è Start Code.
    Auction Date and Time —Ç–µ–ø–µ—Ä—å –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç—Å—è –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É –±–µ–∑ –ª–∏—à–Ω–∏—Ö –ø–µ—Ä–µ–Ω–æ—Å–æ–≤.
    """
    from bs4 import BeautifulSoup
    import re

    soup = BeautifulSoup(path.read_text("utf-8"), "html.parser")
    result = {}

    labels = [
        "Stock #", "Selling Branch", "VIN (Status)", "Loss", "Primary Damage",
        "Title/Sale Doc", "Start Code", "Key", "Odometer", "Airbags",
        "Vehicle", "Body Style", "Engine", "Transmission",
        "Drive Line Type", "Fuel Type", "Cylinders", "Restraint System",
        "Exterior/Interior", "Options", "Manufactured In",
        "Vehicle Class", "Model",
        "Vehicle Location", "Auction Date and Time", "Lane/Run #",
        "Aisle/Stall", "Actual Cash Value", "Estimated Repair Cost", "Seller",
    ]

    for label in labels:
        # –∏—â–µ–º <span class="... data-list__label ...">Label:</span>
        lbl = soup.find(
            lambda tag:
                tag.name == "span"
                and tag.get("class")
                and "data-list__label" in tag.get("class")
                and tag.get_text(strip=True).startswith(f"{label}:")
        )
        if not lbl:
            result[label] = None
            continue

        # –∏—â–µ–º —Å–ª–µ–¥—É—é—â–∏–π tag —Å data-list__value
        val = None
        for sib in lbl.next_siblings:
            if not hasattr(sib, "get"):
                continue
            if "data-list__value" in (sib.get("class") or []):
                text = sib.get_text(" ", strip=True)
                if label == "Auction Date and Time":
                    # —É–¥–∞–ª—è–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã –∏ –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
                    text = " ".join(text.split())
                val = text or None
                break

        result[label] = val

    # –µ—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ Start Code ‚Äî –¥—ë—Ä–Ω–µ–º –∏–∑ —Å–∫—Ä—ã—Ç–æ–≥–æ <input>
    if result.get("Start Code") is None:
        hid = soup.find("input", id="hdnrunAndDrive_Ind")
        novideo = soup.find("span", id="startcodeengine_novideo")
        if novideo:
            result["Start Code"] = novideo.get_text(strip=True)
        elif hid and hid.get("value"):
            result["Start Code"] = hid["value"]

    return result



def extract_photos_from_dimensions(html_path: Path) -> list[str]:
    """
    –ò—â–µ–º –≤ <script> JSON.parse('‚Ä¶') —Å –ø–æ–ª–µ–º "keys":[‚Ä¶] –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–æ –Ω–µ–º—É
    —Å—Å—ã–ª–∫–∏ —á–µ—Ä–µ–∑ resizer —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ width=1000&height=300.
    –ê —Ç–∞–∫–∂–µ –¥–æ–±–∞–≤–ª—è–µ–º 360¬∞-–∫–∞—Ä—Ç–∏–Ω–∫–∏, –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ.
    """
    text = html_path.read_text("utf-8")
    soup = BeautifulSoup(text, "html.parser")

    # 1) –ù–∞–π—Ç–∏ JSON.parse('‚Ä¶') —Å dimensions.keys
    pattern = re.compile(r"JSON\.parse\('(.+?)'\)", re.S)
    raw = None
    for tag in soup.find_all("script"):
        scr = tag.string or ""
        if "dimensions" in scr and "JSON.parse" in scr:
            m = pattern.search(scr)
            if m:
                raw = m.group(1)
                break
    if not raw:
        return []

    dims = json.loads(raw)
    host = "https://vis.iaai.com"

    photos: list[str] = []
    # 2) –ì–µ–Ω–µ—Ä–∏–º –ø–æ –∫–∞–∂–¥–æ–º—É –∫–ª—é—á—É —Ä–æ–≤–Ω–æ –æ–¥–Ω—É —Å—Å—ã–ª–∫—É —Å width=1000&height=300
    for entry in dims.get("keys", []):
        k = entry.get("k")
        if not k:
            continue
        k_quoted = urllib.parse.quote(k, safe="")
        photos.append(
            f"{host}/resizer?imageKeys={k_quoted}&width=1000&height=300"
        )

    # 3) –ï—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω—ã 360¬∞-–∫–∞—Ä—Ç–∏–Ω–∫–∏, –≥–µ–Ω–µ—Ä–∏–º –¥–æ 20 —Å—Å—ã–ª–æ–∫
    if dims.get("image360Ind") and dims.get("image360Url"):
        m = re.search(r"partitionKey=([^&']+)", dims["image360Url"])
        if m:
            pk = m.group(1)
            for i in range(1, 21):
                photos.append(
                    f"https://mediaretriever.iaai.com/ThreeSixtyImageRetriever"
                    f"?tenant=iaai&partitionKey={pk}&imageOrder={i}"
                )

    # 4) –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫
    return list(dict.fromkeys(photos))

def collect_photos(detail_json: dict, partition_key: str, html_path: Path) -> list[str]:
    """
    –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏:
      - –∏–∑ detail_json["Images"] / ["BusinessImages"]
      - –∏–∑ 360¬∞ retriever
      - –∏ –ø–æ dimensions-–±–ª–æ–∫—É –∏–∑ HTML
    """
    photos = []

    # 1) –∏–∑ detail API
    imgs = detail_json.get("Images") or detail_json.get("BusinessImages") or []
    for img in imgs:
        for key in ("LargeUrl","MediumUrl","SmallUrl"):
            url = img.get(key)
            if url and url.startswith("http"):
                photos.append(url)

    # 2) 360¬∞
    if partition_key:
        for i in range(1, 21):
            photos.append(
                f"https://mediaretriever.iaai.com/ThreeSixtyImageRetriever"
                f"?tenant=iaai&partitionKey={partition_key}&imageOrder={i}"
            )

    # 3) –∏–∑ dimensions-–±–ª–æ–∫–∞
    photos.extend(extract_photos_from_dimensions(html_path))

    # —É–±—Ä–∞—Ç—å –¥—É–±–ª–∏
    return list(dict.fromkeys(photos))

def main():
    if len(sys.argv) < 2:
        print("Usage: python fetch_lot_full.py <stockNumber>", file=sys.stderr)
        sys.exit(1)

    stock_id = sys.argv[1]
    html_path = Path("vehicle.html")
    json_path = Path(f"{stock_id}_full.json")

    # 1) HTML
    print("üîÑ Fetching HTML page‚Ä¶")
    ip = fetch_html(stock_id, html_path)
    print(f"üåê Used IP: {ip}")

    # 2) —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø–æ–ª—è
    print("üì¶ Parsing HTML‚Ä¶")
    parsed = parse_html(html_path)

    # 3) AJAX –ø–æ–ª–Ω—ã–π VIN
    print("üì° Fetching AJAX data (full VIN)‚Ä¶")
    try:
        ajax = fetch_vehicle_ajax(stock_id)
        parsed["Full VIN"]   = ajax.get("Vin")
        parsed["Make"]       = ajax.get("MakeName")
        parsed["ModelName"]  = ajax.get("ModelName")
        parsed["ModelYear"]  = ajax.get("ModelYear")
    except Exception as e:
        print("‚ö†Ô∏è AJAX error:", e, file=sys.stderr)

    # 4) —Å–∫—Ä—ã—Ç—ã–π detail API
    print("üì° Fetching detail API data‚Ä¶")
    try:
        detail = fetch_detail_api(stock_id)
    except Exception as e:
        print("‚ö†Ô∏è Detail API error:", e, file=sys.stderr)
        detail = {}

    # 5) —Å–æ–±–∏—Ä–∞–µ–º photos
    print("üì∑ Collecting photos‚Ä¶")
    pk = parsed.pop("_partitionKey360", None)
    parsed["photos"] = collect_photos(detail, pk, html_path)

    # 6) —É–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω–æ–µ –ø–æ–ª–µ
    parsed.pop("_html_path", None)

    # 7) —Å–æ—Ö—Ä–∞–Ω—è–µ–º JSON
    json_path.write_text(json.dumps(parsed, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"‚úÖ Saved full JSON to {json_path}")
    print(json.dumps(parsed, ensure_ascii=False, indent=2))

if __name__ == "__main__":
    main()
